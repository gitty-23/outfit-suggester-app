# -*- coding: utf-8 -*-
"""Model Training.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LapUNK-HzENO5DYfFT-ZtHm-wDNsyNPt
"""

import pandas as pd
import os
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout
from tensorflow.keras.models import Model, load_model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from sklearn.model_selection import train_test_split
import numpy as np
from google.colab import drive
import re

# Initialize
drive.mount('/content/drive')
tf.keras.mixed_precision.set_global_policy('mixed_float16')

# ====================== DATA LOADING ======================
def load_data():
    df_bottomwear = pd.read_csv("/content/drive/MyDrive/results/bottomwear.csv").drop(columns=['Unnamed: 0'], errors='ignore')
    df_topwear = pd.read_csv("/content/drive/MyDrive/results/topwear.csv").drop(columns=['Unnamed: 0'], errors='ignore')

    # Process paths
    for df, img_dir in [(df_bottomwear, "bottomwear"), (df_topwear, "topwear")]:
        df["image_path"] = df["id"].astype(str) + ".jpg"
        df["image_path"] = df["image_path"].apply(
            lambda x: os.path.join(f"/content/drive/MyDrive/results/Images/{img_dir}", x))
        df["exists"] = df["image_path"].apply(os.path.exists)

    df = pd.concat([df_bottomwear, df_topwear], ignore_index=True)
    return df[df["exists"]].drop(columns=["exists"])

# ====================== PREPROCESSING ======================
def preprocess_data(df):
    # Drop any remaining unnamed columns
    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]

    # Enhanced categorical encoding with safe naming
    categorical_cols = ["articleType", "baseColour", "season", "usage"]
    df = pd.get_dummies(df, columns=categorical_cols, prefix_sep="")

    # Clean column names for Keras compatibility
    df.columns = [re.sub(r'[^A-Za-z0-9_]', '_', col) for col in df.columns]
    return df

# ====================== BALANCING ======================
def balance_data(df):
    # Get only valid label columns
    valid_prefixes = ('articleType_', 'baseColour', 'season', 'usage_')
    label_cols = [col for col in df.columns
                 if any(col.startswith(prefix) for prefix in valid_prefixes)]

    targets = {
        'articleType__Jeans': 150,
        'articleType__Tshirts': 100,
        'articleType__Shirts': 80,
        'baseColour__Blue': 80,
        'baseColour__Black': 70,
        'season__Summer': 120,
        'season__Winter': 80
    }

    balanced_dfs = []
    for label, target in targets.items():
        if label in label_cols:
            subset = df[df[label] == 1]
            if len(subset) < target:
                multiplier = (target // len(subset)) + 1
                balanced_dfs.append(pd.concat([subset]*multiplier, ignore_index=True))

    balanced_df = pd.concat([df] + balanced_dfs, ignore_index=True)
    return balanced_df.drop_duplicates(subset=["image_path"])

# ====================== DATA GENERATORS ======================
def create_generators(df):
    # Get valid label columns
    valid_prefixes = ('articleType_', 'baseColour', 'season', 'usage_')
    label_cols = [col for col in df.columns
                 if any(col.startswith(prefix) for prefix in valid_prefixes)]

    train_df, val_df = train_test_split(
        df, test_size=0.2, stratify=df["masterCategory"], random_state=42)

    train_datagen = ImageDataGenerator(
        rescale=1./255,
        rotation_range=25,
        width_shift_range=0.2,
        height_shift_range=0.2,
        shear_range=0.2,
        zoom_range=0.2,
        horizontal_flip=True,
        brightness_range=[0.8, 1.2]
    )

    val_datagen = ImageDataGenerator(rescale=1./255)

    train_gen = train_datagen.flow_from_dataframe(
        dataframe=train_df,
        x_col="image_path",
        y_col=label_cols,
        target_size=(160, 160),
        batch_size=32,
        class_mode="raw",
        shuffle=True
    )

    val_gen = val_datagen.flow_from_dataframe(
        dataframe=val_df,
        x_col="image_path",
        y_col=label_cols,
        target_size=(160, 160),
        batch_size=32,
        class_mode="raw",
        shuffle=True
    )

    return train_gen, val_gen, label_cols

# ====================== MODEL BUILDING ======================
def build_model(num_classes):
    base_model = MobileNetV2(
        weights="imagenet",
        include_top=False,
        input_shape=(160, 160, 3)
    )

    # Fine-tuning
    for layer in base_model.layers[:100]:
        layer.trainable = False
    for layer in base_model.layers[100:]:
        layer.trainable = True

    # Architecture
    x = base_model.output
    x = GlobalAveragePooling2D()(x)
    x = Dense(256, activation='relu')(x)
    x = Dropout(0.5)(x)

    outputs = Dense(num_classes, activation='sigmoid')(x)

    model = Model(inputs=base_model.input, outputs=outputs)

    model.compile(
        optimizer=Adam(learning_rate=1e-4),
        loss='binary_crossentropy',
        metrics=['accuracy']
    )

    return model



# ====================== TRAINING ======================
def train_model():
    print("Starting training pipeline...")
    print("Loading data...")
    raw_df = load_data()

    print("Preprocessing data...")
    processed_df = preprocess_data(raw_df)

    print("Balancing data...")
    balanced_df = balance_data(processed_df)

    print("Creating generators...")
    train_gen, val_gen, label_columns = create_generators(balanced_df)

    print("\nBuilding model...")
    model = build_model(len(label_columns))

    callbacks = [
        EarlyStopping(patience=7, restore_best_weights=True),
        ReduceLROnPlateau(factor=0.1, patience=3)
    ]

    print("\nTraining model...")
    history = model.fit(
        train_gen,
        validation_data=val_gen,
        epochs=30,
        callbacks=callbacks,
        steps_per_epoch=len(train_gen),
        validation_steps=len(val_gen)
    )

    model.save("/content/drive/MyDrive/optimized_fashion_classifier5.0.keras")
    return model, history, label_columns

# Execute training
model, history, label_columns, val_gen = train_model()

print("Training completed successfully!")

# ====================== PREDICTION ======================
def predict_fashion(image_path, model_path, label_columns):
    model = load_model(model_path)
    img = load_img(image_path, target_size=(160, 160))
    img_array = img_to_array(img) / 255.0
    img_array = np.expand_dims(img_array, axis=0)

    preds = model.predict(img_array)[0]

    results = {}
    for i, col in enumerate(label_columns):
        results[col] = "Yes" if preds[i] > 0.5 else "No"

    return results

# Example prediction
sample_pred = predict_fashion(
    "/content/drive/MyDrive/results/Images/bottomwear/11281.jpg",
    "/content/drive/MyDrive/optimized_fashion_classifier5.0.keras",
    label_columns
)
print("Prediction:", sample_pred)











import matplotlib.pyplot as plt

# Plot Accuracy
plt.figure(figsize=(10, 5))
plt.plot(history.history['accuracy'], label='Train Accuracy', marker='o')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy', marker='o')
plt.title('Training vs Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)
plt.show()

# Plot Loss
plt.figure(figsize=(10, 5))
plt.plot(history.history['loss'], label='Train Loss', marker='o')
plt.plot(history.history['val_loss'], label='Validation Loss', marker='o')
plt.title('Training vs Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.show()

raw_df = load_data()
processed_df = preprocess_data(raw_df)

import seaborn as sns
import matplotlib.pyplot as plt

# ---------- Class Distributions Before Balancing ----------
def plot_class_distributions(df, title_prefix=""):
    fig, axs = plt.subplots(2, 2, figsize=(18, 10))
    sns.countplot(data=df, x='articleType', ax=axs[0, 0])
    axs[0, 0].set_title(f'{title_prefix} Article Type Distribution')
    axs[0, 0].tick_params(axis='x', rotation=45)

    sns.countplot(data=df, x='baseColour', ax=axs[0, 1])
    axs[0, 1].set_title(f'{title_prefix} Base Colour Distribution')
    axs[0, 1].tick_params(axis='x', rotation=45)

    sns.countplot(data=df, x='season', ax=axs[1, 0])
    axs[1, 0].set_title(f'{title_prefix} Season Distribution')
    axs[1, 0].tick_params(axis='x', rotation=45)

    sns.countplot(data=df, x='usage', ax=axs[1, 1])
    axs[1, 1].set_title(f'{title_prefix} Usage Distribution')
    axs[1, 1].tick_params(axis='x', rotation=45)

    plt.tight_layout()
    plt.show()

# Plot before balancing
plot_class_distributions(raw_df, title_prefix="Before Balancing")

# Plot after balancing
balanced_df = balance_data(processed_df)
plot_class_distributions(balanced_df, title_prefix="After Balancing")

from sklearn.metrics import confusion_matrix, roc_curve, auc, precision_recall_curve
import numpy as np
import itertools

# Get true and predicted labels from validation set
def get_preds_from_generator(model, generator, label_columns):
    y_true = []
    y_pred = []

    for i in range(len(generator)):
        x_batch, y_batch = generator[i]
        y_true.append(y_batch)
        preds = model.predict(x_batch)
        y_pred.append(preds)

    y_true = np.vstack(y_true)
    y_pred = np.vstack(y_pred)
    return y_true, y_pred

y_true, y_pred = get_preds_from_generator(model, val_gen, label_columns)

import matplotlib.pyplot as plt

def plot_training_curves(history):
    acc = history.history['accuracy']
    val_acc = history.history['val_accuracy']
    loss = history.history['loss']
    val_loss = history.history['val_loss']
    epochs = range(1, len(acc) + 1)

    plt.figure(figsize=(14, 5))

    # Accuracy
    plt.subplot(1, 2, 1)
    plt.plot(epochs, acc, 'bo-', label='Training Accuracy')
    plt.plot(epochs, val_acc, 'ro-', label='Validation Accuracy')
    plt.title('Training and Validation Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()

    # Loss
    plt.subplot(1, 2, 2)
    plt.plot(epochs, loss, 'bo-', label='Training Loss')
    plt.plot(epochs, val_loss, 'ro-', label='Validation Loss')
    plt.title('Training and Validation Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()

    plt.tight_layout()
    plt.show()

# Call after training
plot_training_curves(history)

from sklearn.metrics import roc_curve, auc
from sklearn.preprocessing import label_binarize

def plot_roc_multilabel(model, val_gen, label_columns):
    y_true = []
    y_score = []

    for i in range(len(val_gen)):
        x_batch, y_batch = val_gen[i]
        preds = model.predict(x_batch)
        y_true.append(y_batch)
        y_score.append(preds)

        if i == 10:  # Limit to 10 batches for speed
            break

    y_true = np.concatenate(y_true)
    y_score = np.concatenate(y_score)

    plt.figure(figsize=(10, 8))

    for i in range(len(label_columns)):
        fpr, tpr, _ = roc_curve(y_true[:, i], y_score[:, i])
        roc_auc = auc(fpr, tpr)
        plt.plot(fpr, tpr, label=f'{label_columns[i]} (AUC = {roc_auc:.2f})')

    plt.plot([0, 1], [0, 1], 'k--', lw=1)
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curves for Multi-label Classification')
    plt.legend(loc='lower right', fontsize='small')
    plt.grid()
    plt.show()

# Call after training
plot_roc_multilabel(model, val_gen, label_columns)

















